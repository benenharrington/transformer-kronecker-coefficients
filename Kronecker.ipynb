{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import KroneckerModel\n",
    "import sys  \n",
    "sys.path.insert(0, '../symchar')\n",
    "from symchar import Partition, character_table\n",
    "from math import factorial\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_for_character_table_i(i, character_table_dict, f):\n",
    "    i_table = character_table_dict[i]\n",
    "    for mu_partition in i_table.partition_lookup:\n",
    "        for nu_partition in i_table.partition_lookup:\n",
    "            kronecker_coefficients = i_table.kronecker_coefficients(mu_partition, nu_partition)\n",
    "            for lambda_partition, value in kronecker_coefficients.items():\n",
    "                if value > 0: \n",
    "                    f.write(' '.join([str(part) for part in mu_partition]) + '  ' + ' '.join([str(part) for part in nu_partition]) + '  ' + ' '.join([str(part) for part in lambda_partition]) + '  1\\n')\n",
    "                else:\n",
    "                    f.write(' '.join([str(part) for part in mu_partition]) + '  ' + ' '.join([str(part) for part in nu_partition]) + '  ' + ' '.join([str(part) for part in lambda_partition]) + '  0\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_groups = [i for i in range(3, 16, 2)]\n",
    "training_groups = [1] + [i for i in range(2, 16, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting character tables\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "Preparing training data\n",
      "1\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "Preparing validation data\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "character_table_dict = {}\n",
    "print(\"Getting character tables\")\n",
    "for i in training_groups + validation_groups:\n",
    "    print(i)\n",
    "    character_table_dict[i] = character_table(i)\n",
    "    \n",
    "print(\"Preparing training data\")\n",
    "with open('kron_training_data.txt', 'w') as f:\n",
    "    for i in training_groups:\n",
    "        print(i)\n",
    "        write_data_for_character_table_i(i, character_table_dict, f)\n",
    "        \n",
    "print(\"Preparing validation data\")\n",
    "with open('kron_validation_data.txt', 'w') as f:\n",
    "    for i in validation_groups:\n",
    "        print(i)\n",
    "        write_data_for_character_table_i(i, character_table_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARTITION_LENGTH = 10\n",
    "MAX_INPUT_LENGTH = 3*MAX_PARTITION_LENGTH\n",
    "MAX_PART_VALUE = 12\n",
    "\n",
    "def line_to_data(line):\n",
    "    partition_1, partition_2, partition_3, ground_truth = line.split('  ')\n",
    "    partition_1, partition_2, partition_3 = [[int(item)-1 for item in partition.split(' ')] for partition in [partition_1, partition_2, partition_3]]\n",
    "    lengths = [len(partition) for partition in [partition_1, partition_2, partition_3]]\n",
    "    if any(length > 10 for length in lengths) or any(value >= MAX_PART_VALUE for value in [partition_1[0], partition_2[0], partition_3[0]]):\n",
    "        return [None]*5\n",
    "    total_lengths = sum(lengths)\n",
    "    mask = [1]*total_lengths + [0]*(MAX_INPUT_LENGTH - total_lengths)\n",
    "    sequence_positions = lengths[0]*[1] + lengths[1]*[2] + lengths[2]*[3] + [0]*(MAX_INPUT_LENGTH - total_lengths)\n",
    "    term_positions = [i for i in range(0, lengths[0])] + [i for i in range(0, lengths[1])] + [i for i in range(0, lengths[2])] + [0]*(MAX_INPUT_LENGTH - total_lengths)\n",
    "    partition_data = partition_1 + partition_2 + partition_3 + [0]*(MAX_INPUT_LENGTH - total_lengths)\n",
    "    ground_truth = int(ground_truth[0])\n",
    "    return partition_data, sequence_positions, term_positions, ground_truth, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('kron_training_data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "all_partition_data, all_sequence_positions, all_term_positions, all_ground_truths, all_masks = [], [], [], [], []\n",
    "for line in lines:\n",
    "    partition_data, sequence_positions, term_positions, ground_truth, mask = line_to_data(line)\n",
    "    if partition_data is not None:\n",
    "        all_partition_data.append(partition_data)\n",
    "        all_sequence_positions.append(sequence_positions)\n",
    "        all_term_positions.append(term_positions)\n",
    "        all_ground_truths.append(ground_truth)\n",
    "        all_masks.append(mask)\n",
    "        \n",
    "import torch\n",
    "x_partition_data = torch.tensor(all_partition_data)\n",
    "x_sequence_positions = torch.tensor(all_sequence_positions)\n",
    "x_term_positions = torch.tensor(all_term_positions)\n",
    "x_masks = torch.tensor(all_masks, dtype=float)\n",
    "y_data = torch.tensor(all_ground_truths)\n",
    "TRAIN_SIZE = len(y_data)\n",
    "train_shuffle = np.random.choice(TRAIN_SIZE, size=TRAIN_SIZE, replace=False)\n",
    "x_partition_train, x_sequence_train, x_term_train, x_masks_train, y_train = x_partition_data[train_shuffle], x_sequence_positions[train_shuffle], x_term_positions[train_shuffle], x_masks[train_shuffle], y_data[train_shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0\n",
    "with open('kron_validation_data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "validation_dict = {i:{} for i in validation_groups}\n",
    "current_first_partition = '3'\n",
    "current_second_partition = '3'\n",
    "current_group = 3\n",
    "current_dictionary = validation_dict[3]\n",
    "all_partition_data, all_sequence_positions, all_term_positions, all_ground_truths, all_masks = [], [], [], [], []\n",
    "for line in lines:\n",
    "    line_first_partition, line_second_partition = line.split('  ')[:2]\n",
    "    if line_second_partition != current_second_partition:\n",
    "        x_partition_data = torch.tensor(all_partition_data)\n",
    "        x_sequence_positions = torch.tensor(all_sequence_positions)\n",
    "        x_term_positions = torch.tensor(all_term_positions)\n",
    "        x_masks = torch.tensor(all_masks, dtype=float)\n",
    "        y_data = torch.tensor(all_ground_truths)\n",
    "        current_dictionary[(current_first_partition, current_second_partition)] = (x_partition_data, x_sequence_positions, x_term_positions, x_masks, y_data)\n",
    "        line_first_digit = int(line_first_partition.split(' ')[0])\n",
    "        if line_first_digit > current_group:\n",
    "            current_group = line_first_digit\n",
    "            current_dictionary = validation_dict[current_group]\n",
    "        current_first_partition = line_first_partition\n",
    "        current_second_partition = line_second_partition\n",
    "        all_partition_data, all_sequence_positions, all_term_positions, all_ground_truths, all_masks = [], [], [], [], []\n",
    "    partition_data, sequence_positions, term_positions, ground_truth, mask = line_to_data(line)\n",
    "    if partition_data is not None:\n",
    "        all_partition_data.append(partition_data)\n",
    "        all_sequence_positions.append(sequence_positions)\n",
    "        all_term_positions.append(term_positions)\n",
    "        all_ground_truths.append(ground_truth)\n",
    "        all_masks.append(mask)\n",
    "        VAL_SIZE += 1\n",
    "x_partition_data = torch.tensor(all_partition_data)\n",
    "x_sequence_positions = torch.tensor(all_sequence_positions)\n",
    "x_term_positions = torch.tensor(all_term_positions)\n",
    "x_masks = torch.tensor(all_masks, dtype=float)\n",
    "y_data = torch.tensor(all_ground_truths)\n",
    "current_dictionary[(current_first_partition, current_second_partition)] = (x_partition_data, x_sequence_positions, x_term_positions, x_masks, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = KroneckerModel(num_tokens=MAX_PART_VALUE, input_embedding_dim=200, vector_dim=200, num_heads=10, num_layers=4, hidden_multiplier=1, dropout_prob=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
    "\n",
    "model.train()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "stats = []\n",
    "val_stats = []\n",
    "solved_partitions = []\n",
    "for i in range(0, num_epochs):\n",
    "    print(i)\n",
    "    total_acc = 0\n",
    "    temp_acc = 0\n",
    "    for run, j in enumerate(range(0, TRAIN_SIZE, batch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_partition_train[j:j + batch_size], x_sequence_train[j:j + batch_size], x_term_train[j:j + batch_size], x_masks_train[j:j + batch_size])\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        loss_function = nn.BCELoss()\n",
    "        out = sigmoid(out.mean(axis=1).squeeze(1))\n",
    "        loss = loss_function(out.to(torch.float32), y_train[j:j + batch_size].to(torch.float32))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        batch_acc = sum(torch.logical_and(out > 0.5, y_train[j:j + batch_size] == 1)) + sum(torch.logical_and(out< 0.5, y_train[j:j + batch_size] == 0))\n",
    "        total_acc += batch_acc\n",
    "        temp_acc += batch_acc\n",
    "    print(\"Train accuracy:\", total_acc/TRAIN_SIZE)\n",
    "    stats.append(total_acc/TRAIN_SIZE)\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    for group, group_dictionary in validation_dict.items():\n",
    "        group_acc = 0\n",
    "        group_items = 0\n",
    "        for partition, values in group_dictionary.items():\n",
    "            if values[0].shape[0] > 0:\n",
    "                num_items = values[0].shape[0]\n",
    "                partition_acc = 0\n",
    "                for j in range(0, num_items, batch_size):\n",
    "                    out = model(values[0][j:j+batch_size], values[1][j:j+batch_size], values[2][j:j+batch_size], values[3][j:j+batch_size])\n",
    "                    out = sigmoid(out.mean(axis=1).squeeze(1))\n",
    "                    partition_acc += sum(torch.logical_and(out > 0.5, values[4][j:j+batch_size] == 1)) + sum(torch.logical_and(out< 0.5, values[4][j:j+batch_size] == 0))\n",
    "                if partition_acc == num_items and partition not in solved_partitions:\n",
    "                    print(partition)\n",
    "                    solved_partitions.append(partition)\n",
    "                group_items += num_items\n",
    "                group_acc += partition_acc\n",
    "        print(group, group_acc/group_items)\n",
    "        val_acc += group_acc\n",
    "    print(\"Validation accuracy:\", val_acc/VAL_SIZE)\n",
    "    val_stats.append(val_acc/VAL_SIZE)\n",
    "    torch.save(model.state_dict(), f\"{i}_kron_writeup_long_run.blob\")\n",
    "    model.train()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"8_kron_writeup_long_run.blob\"))\n",
    "model.eval()\n",
    "\n",
    "def line_to_torch_input(line):\n",
    "    partition_data, sequence_positions, term_positions, ground_truth, mask = line_to_data(line)\n",
    "    return [torch.tensor(item).unsqueeze(0) for item in [partition_data, sequence_positions, term_positions]] + [torch.tensor(mask, dtype=float).unsqueeze(0)]\n",
    "\n",
    "import numpy as np\n",
    "p = []\n",
    "with open('kron_validation_data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    if line.split(\"  \")[0] == '4 3 2 2 1 1 1 1':\n",
    "        p.append(line)\n",
    "        \n",
    "good = []\n",
    "for line in p:\n",
    "    try:\n",
    "        e = line_to_torch_input(line)\n",
    "        good.append(line)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
